{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance Model Development through Evaluation\n",
    "- Test performance across different demensions, such as mathmatics, reasoning, logics.\n",
    "- Adapability in different fields, such as medical, finance, legal and etc.\n",
    "- Adapability to different languages (i.e. Chinese)\n",
    "- Feedback iteration via evaluation\n",
    "\n",
    "# Challenges of LLM\n",
    "- Completeness to cover different environment\n",
    "- Cost of reasoning and evaluation, especially if via mannual evaluate\n",
    "- Data cleanse, requires tech skills to detect the dirty data and dynamicly update high-quaity evaluation standards\n",
    "- LLM is too sensitive to the provided key words, performance is not stable with it\n",
    "\n",
    "# How do we evaluate an LLM\n",
    "- Different evaluate approaches for different models\n",
    "    - Base model: for next token prediction\n",
    "    - Open source model (with XTuner and RAG)\n",
    "    - API model\n",
    "- Subjective evaluation (poems, art) vs. objective evaluation (knowledge point q&a)\n",
    "- Hint words, to enrich the context, provide guidance, details and feedbacks\n",
    "- Long context evaluation, information extraction\n",
    "\n",
    "# Toolkits\n",
    "- VLMEvalKit: for multimodal evaluation\n",
    "- Code-Evaluator: stability and repeateablity of code evaluation\n",
    "- MixtralKit: toolkits for beginners of MoE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCompass Project\n",
    "https://github.com/InternLM/Tutorial/blob/main/opencompass/opencompass_tutorial.md\n",
    "### Virtual machine setup\n",
    "- Mirror: Cuda11.7-conda\n",
    "- GPU: 10% A100\n",
    "\n",
    "### Environment setup\n",
    "- Install and activate opencompass environment\n",
    "- Prepare the dataset to be evaluated\n",
    "![alt text](../images/opencompass_hw_pic1.png)\n",
    "\n",
    "### Evaluation\n",
    "- option 1: python run.py --datasets ceval_gen --hf-path \\[path to the base model\\] --tokenizer-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True --model-kwargs trust_remote_code=True device_map='auto' --max-seq-len \\[maxmimun length of the input token\\] --max-out-len \\[maximun length of output token\\] --batch-size 2 --num-gpus 1 --debug (show bug info on terminal) --work_dir \\[path where opencompass runs, default /outputs/default \\] --reuse latest (reuse the latest work_dir)\n",
    "- option 2: python run.py configs/eval_demo.py\n",
    "![alt text](../images/opencompass_hw_pic2.png)\n",
    "\n",
    "### Hints for OpenCompass run.py\n",
    "- Under repository, /opencompass/opencompass, it defines the logic how to run partitioned tasks\n",
    "    - partitions: partition the task to each of the GPUs\n",
    "    - tasks: defines how the partitioned tasks are to be submitted\n",
    "    - runners: how the task is executed, on local GPU or clusters\n",
    "    - openicl: involves logic of reasoning and evaluation\n",
    "    - summarizers: how the model is to be rated\n",
    "- Evaluate custom datasets\n",
    "    - configs/datasets/\\[ceval\\]/: generate a new py file, need to return a list of ceval datasets. In ceval_gen.py, update the lib name to the newly created py file\n",
    "        - dict: reader_cfg, infer_cfg, eval_cfg\n",
    "        - It should import the correct dataset class in step 2\n",
    "        - update type value in its retured list of dict. \"path\" is the root directory of ceval involing all sub-modules. \"name\" is the name of the submodule.\n",
    "    - opencompass/datasets: generate a new class, for example ceval2.py. Implement static method load()\n",
    "    - opencompass/datasets/\\_\\_init\\_\\_.py: import the new class generated in step 2\n",
    "- Other evaluations\n",
    "    - Subjective evaluation: reference config/datasets/eval_subjective_\\[xxx\\].py\n",
    "    - Needlebench evaluation: reference config/datasets/eval_needlebench.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
