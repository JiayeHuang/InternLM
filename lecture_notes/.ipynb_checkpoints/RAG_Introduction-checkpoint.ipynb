{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RAG\n",
    "\n",
    "- RAG (Retrieval Augmented Generation): a tech combined of both retrieval and generation. It enhances LLMs function via external knowledge bases, by searching the user's input information piece and generating a more accurate answer.\n",
    "- It's compatible of knowledge intensive tasks.\n",
    "- To keep its knowledge base up to date, long-tail knowledges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work\n",
    "- Indexing: spliting the external knowledge source into chunks, indexing and saving it into the vector database\n",
    "- Retrieval: receiving questions from the user, indexing it. Matching the generated index with the index from the vector database, finding the most related top-k chunks.\n",
    "- Generation: inputting the question and the top-k chunks to the LLM model to generate the final answer\n",
    "\n",
    "### Vector database\n",
    "- Converting the context into vectors of specific length, which could reflect the info from the context\n",
    "- Using cos to find the most relevant vector as per users' input\n",
    "- Optimisation involves better context encoding tech, sentence/paragraph embedding and optimisation of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of RAG\n",
    "- Naive RAG, makes up of retrieval, indexing and generation. Used for Q&A and searching systems\n",
    "- Advanced RAG, involves pre-retrieval to inprove the quality of user's input, and post-retrieval to re-rank and summary the searched info. Used to generate summary and content recommendation.\n",
    "- Modular RAG: modular RAG functions, to customize the RAG model per user's requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG optimization\n",
    "### Improve Vector database\n",
    "- Embedding Optimization: 稀疏和密集检索，multi-tasks (I don't understand it.)\n",
    "- Indexing Optimization: optimize size of chunk, meta data\n",
    "### Optimize query\n",
    "- Extend or convert the user's input question, multi-queries\n",
    "- Context Curation: rerank context selection/compression\n",
    "### Optimize Retrieval\n",
    "- Iterative Retrieval: Iteratively searching the initial input and subsequent inputs\n",
    "- Recursive Retrieval: Narrow down the search via iterative search. Use Chain-of-Thought to guide the retrieval process\n",
    "- Adaptive Retrieval: Use LLMs model to determine the best retrieval context\n",
    "### LLM Fine-tuning\n",
    "- Train on specific datasets to have it better suit the mission. Requires large amounts of labeled data\n",
    "- Adapts to high-prefossional fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Framework and Tests\n",
    "- Evaluation Index: Accuracy, Recall, F1 Score, BLEU Score, ROUGE Score\n",
    "- Test Framework: RGB, RECALL, CRUD\n",
    "- Evaluation Tool: RAGAS, ARES, TruLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application - HuiXiangDou\n",
    "- Developed by InternLM, a knowledge assistant using LLMs. It provides tech support and automatical q&a in wechat discussion group.\n",
    "- Deployment: https://github.com/InternLM/Tutorial/tree/camp2/huixiangdou\n",
    "\n",
    "### huixiangdou deployment\n",
    "- Add machine: 30% A100\n",
    "- Mirror: Cuda 11.7-conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
