{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deploy and run the trained model in specific environment\n",
    "- In CPU, GPU or NPU?; mobile device?; multi-GPU, cluster deployment?\n",
    "\n",
    "# Challenges\n",
    "- Amounts of parameters for calculation (20B model with 48 layers, C(forward)~40.6GFlops)\n",
    "- VRAM consumption\n",
    "- Memory access limitation due to the dynamic requests\n",
    "\n",
    "# Existing Deployment Methods\n",
    "### Pruning: reducing the size of a model by removing less important or redundant parameters\n",
    "- Structured pruning (i.e. LLM-Pruner): pruning removes entire neurons, convolutional filters, channels, or even layers, rather than individual weights.\n",
    "- Unstructured pruning (i.e. SparseGPT, LoRAPrune, Wanda): removing individual weights from the model without considering their specific locations within neurons, channels, or layers.\n",
    "\n",
    "### Knowledge Distillation\n",
    "- A smaller, more efficient model (the student) is trained to replicate the behavior of a larger, more complex model (the teacher)\n",
    "\n",
    "### Quantization\n",
    "- Convert float to int or other discrete forms （Involves steps of quantization and dequantization. It takes more calc rounds, but saves I/O time)\n",
    "- What scenario is each of quantization method for?? Their advantages and disadvantages?? And real world user case??\n",
    "- QAT (LLM-QAT): train neural networks with the intention of deploying them in a quantized form\n",
    "- QAF (PEQA, QLORA)：a pre-trained model undergoes fine-tuning while considering the effects of quantization\n",
    "- PTQ (LLM.int8, AWQ)：a fully trained model is quantized after the training process has been completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMDeploy Solutions\n",
    "- Model Efficient Inference (Imdeploy chat -h)：executing models in a manner that maximizes computational efficiency. Supports LLaMA model, continuous batch mode (for dynamic requests), and scalable KV cache manager (take our the cache not in use from VRAM to memory, to decrease the occupation of VRAM)\n",
    "- Model Quantization Compression (Imdeploy lite -h): quantisize the weitht of model from FP16 to INT4 to reduce the I/O amount and dequantisize it during kernel calculation\n",
    "- Service Oriented Deploy (Imdeploy serve -h): encrypt as of HTTP API\n",
    "- Supports multimodal large model (LLaVA) using pipeline, which supports image input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "- Deploying LMDeoply model on the virtual machine to chat with InternLM2-Chat-1.8B using cmd\n",
    "- https://github.com/InternLM/Tutorial/blob/camp2/lmdeploy/README.md\n",
    "\n",
    "### Virtual machine requirements\n",
    "- GPU: 10% A100*1\n",
    "- Mirror: Cuda12.2-conda\n",
    "\n",
    "### Setup environment\n",
    "- Setup lmdeploy environment and activate it\n",
    "- Install lmdeploy\n",
    "![alt text](../images/lmdeploy_hw_pic1.png)\n",
    "\n",
    "### LMDeploy chat model （i.e. InternLM2-Chat-1.8B)\n",
    "- HuggingFace: Opensource society to share models and codes, in forms of HuggingFace. China local has similar socitiest such as MindScope (Alibaba) and OpenXLab (Shanghai AI Lab)\n",
    "- TurboMind：A high-efficiency inference engine for LLM (Large Language Models) developed by the LMDeploy team. Its main features include support for LLaMa structured models, a continuous batch inference mode, and a scalable KV (Key-Value) cache manager. It's a submodule of LMDeploy, and LMDeploy can be inferenced with other modules such as pytorch. TurboMind engine is capable of performing inference only on models formatted specifically for TurboMind.\n",
    "- Download chat model InternLM2-Chat-1.8B\n",
    "- Compare the performance using transformer lib or LMDeploy\n",
    "![alt text](../images/lmdeploy_hw_pic2.png)\n",
    "![alt text](../images/lmdeploy_hw_pic3.png)\n",
    "![alt text](../images/lmdeploy_hw_pic4.png)\n",
    "\n",
    "### Quantization practices\n",
    "- Maximize KV cache occupation rate to 0.4, triggger W4A16 quantization\n",
    "- First install einops==0.7.0 to enable W4A16 quantization\n",
    "![alt text](../images/lmdeploy_hw_pic5.png)\n",
    "![alt text](../images/lmdeploy_hw_pic6.png)\n",
    "![alt text](../images/lmdeploy_hw_pic7.png)\n",
    "\n",
    "### Trigger Imdeploy with API Server\n",
    "- With bash API-Client\n",
    "![alt text](../images/lmdeploy_hw_pic8.png)\n",
    "![alt text](../images/lmdeploy_hw_pic9.png)\n",
    "\n",
    "- With web Gradio API-Client\n",
    "![alt text](../images/lmdeploy_hw_pic10.png)\n",
    "![alt text](../images/lmdeploy_hw_pic11.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
