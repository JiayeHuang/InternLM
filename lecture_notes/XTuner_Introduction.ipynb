{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Fine-tuning\n",
    "- To fit the requirement in specific professional fields\n",
    "- The limitations from prompt engineering: requires too much input, output is not ideal\n",
    "- To deploy locally for data safety\n",
    "- Customized service\n",
    "\n",
    "# Target of Fine-tuning\n",
    "- To eliminate illusions from the model, improve response to instructions, work in specific professional fields\n",
    "- Protect data safety and privacy\n",
    "- Reduce training cose and improve training efficiency. Dynamicly learning.\n",
    "- Improve reliability and response time\n",
    "\n",
    "# Two Fine-tuning models\n",
    "#### Incremental Pretraining\n",
    "- Teach the base model a new knowledge，for specific professional fields\n",
    "- Training object: article, book, code\n",
    "\n",
    "#### Instruction Tuning\n",
    "- Enable the model to learn the chat model, to communicate with humans per instructions\n",
    "- Training object: q&a, high quality communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a piece of data\n",
    "- Raw data: involves system settinngs, user input and robot output\n",
    "- Formalize data: convert the raw data in format of json\n",
    "- Add dialog template: to identify if the words are from system, user or robot. Different models apply different templates.\n",
    "- Tokenize data\n",
    "- Add label\n",
    "- Start training: only calculating loss by comparing robot output and expected output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Strategy\n",
    "### Fine-tuning\n",
    "- It involves base model in the training. It is necessary to save the optimizer state for the parameters in the base model.\n",
    "\n",
    "### LoRA (Low rank adaptions of large language model)\n",
    "- Add a new branch next to the original linear layer, consisting of two consecutive smaller linear layers. This new branch is called adapter. Adapter parameters are much smaller than the original linear layer, significantly reducing memory consumption during training\n",
    "- Base model is only involved in forward training. Only the Adapter parameters are updated in backward training. Only optimizer state for parameters from Adapters are required to be saved.\n",
    "\n",
    "### QLoRA\n",
    "- Improvement based on LoRA\n",
    "- Quantize the model size to 4-bit: reduce the precision of the model's weights and activations to 4 bits, which can significantly reduce the model's size and computational requirements, often with minimal impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to take for fine tuning\n",
    "- Clarify target application and task\n",
    "- Select a base model\n",
    "- Prepare data: collect, clean, pre-process, label, slice\n",
    "    - Data collection: Online, public datasets(Alpaca, Self-instruct, LIMA, Dolly...), manual label\n",
    "    - Data enforcement: rephrase data inputs, convert context to dialogs\n",
    "    - Convert data format: via XTuner, divide training and test sets\n",
    "\n",
    "###### Subsequent steps are supported by XTuner\n",
    "- Set fine tuning strategy: LoRA, QLOra\n",
    "- Set parameters: batch size, evaluation_inputs, evaluation_freq...\n",
    "- Initialize the model\n",
    "- Start fine tuning\n",
    "###### -----\n",
    "- Evaluate model and adjust parameters\n",
    "- Test model performance\n",
    "- Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternLM Model - XTuner\n",
    "- Compatible with various fine-tuning algorithms, covering a wide range of SFT scenarios\n",
    "- Support HuggingFace Hub (utility based models), ModelScope\n",
    "- Equip with map functions to transform format of datasets and dialog templates\n",
    "- Require 8GB GPU memory\n",
    "\n",
    "### Deploy\n",
    "- Install xtuner and pick appropriate configuration template: xtuner list-cfg -p [model_base]\n",
    "- Copy configuration template: xtuner copy-cfg [model_base]\\_[algo]\\_[data_sets]\\_[data length]\\_[Epoch] /dest/path\n",
    "- Edit configuration template: data path or warehouse name, max length(maximum token number), pack to max length（boolean, whether to concat multi tokens to save, accumulative counts, evaluation inputs, evaluation freq\n",
    "- Start training: xtuner train [model_base]\\_[algo]\\_[data_sets]\\_[data length]\\_[Epoch]\\_copy.py\n",
    "\n",
    "### Chat\n",
    "- float 16: xtuner chat path/model_name\n",
    "- 4bit: xtuner chat path/model_name --bits 4\n",
    "- load adapter model: xtuner chat path/model_name --adapter [adapter_dir]\n",
    "\n",
    "### Optimization Strategy to deploy Xtuner with 8GB GPU memory\n",
    "#### Flash Attention (auto triggered)\n",
    "- Parallel Attention Calculation (what is attention calculation? what's it for??), to reduce the N\\*N VRAM occupization when calculating Attention Score\n",
    "\n",
    "#### DeepSpeed ZeRO (triggered with \\-\\-deepspeed deepspeed_zero3)\n",
    "- Slice and save parameters, gradients, and optimizer states during training to save VRAM when training on multiple GPUs\n",
    "- Use FP16 weights with DeepSpeed training significantly saves VRAM on a single GPU compared to PyTorch's AMP training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XTuner Project with QLoRA\n",
    "\n",
    "- Mirror: Cuda 11.7 Conda\n",
    "- GPU: 10% A100\n",
    "\n",
    "### Steps: \n",
    "- https://github.com/InternLM/Tutorial/blob/camp2/xtuner/personal_assistant_document.md\n",
    "\n",
    "#### Environment preparament\n",
    "- Install xtuner and activate: studio-conda xtuner0.1.17, conda activate xtuner0.1.17\n",
    "- Clone xtuner coda and install from cloned code\n",
    "![alt text](../images/xtuner_llm_hw_pic1.png)\n",
    "\n",
    "- Prepare training data in /root/ft/data/personal_assistant.json\n",
    "- Prepare model: download from OpenXLab or Modelscope. In this project, it has been pre-installed, so just link from the root of virtual machine.\n",
    "\n",
    "#### Configuration file setup\n",
    "- list the supported confiduration template and copy the approriate one to /root/ft/config\n",
    "![alt text](../images/xtuner_llm_hw_pic2.png)\n",
    "- Edit Configuration template\n",
    "\n",
    "#### Start training\n",
    "- Can alternatively use deep-speed, if your local machine has a limited GPU source\n",
    "![alt_text](../images/xtuner_llm_hw_pic3.png)\n",
    "![alt_text](../images/xtuner_llm_hw_pic4.png)\n",
    "\n",
    "#### Convertion, integration, test and deployment\n",
    "- Convert the model weight files originally trained using PyTorch to the commonly used Huggingface format\n",
    "- Integrate the trained adapter with the original model\n",
    "![alt text](../images/xtuner_llm_hw_pic5.png)\n",
    "- chat\n",
    "![alt text](../images/xtuner_llm_hw_pic6.png)\n",
    "\n",
    "#### Deploy to the web\n",
    "- Install streamlit, clone InternLM code\n",
    "- Use powershell ssh to the virtual machine\n",
    "- Run web demo: streamlit run /root/ft/web_demo/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternLM2 1.8B Model\n",
    "- InternLM2-Chat-1.8B-SFT: chat model by applying supervised fine-tuning (SFT) on InternLM2-1.8B\n",
    "- InternLM2-Chat-1.8B (3.78GB): improve the model alignment and performance through Reinforcement Learning from Human Feedback (RLHF) techniques applied after initial supervised fine-tuning (SFT) on the InternLM2-Chat-1.8B model\n",
    "- Under precision FP16, InternLM2-1.8B requires only 4GB VRAM to run on a pc, and 8GB VRAM to fine tune the model (beginner friendly) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal LLM\n",
    "- For multiple types of data inputs, to perform tasks that require integrating information from these different modalities.\n",
    "\n",
    "### LLaVA - enhance visual capability for LLM models\n",
    "- Use GPT-4V generating image descriptions, in terms of \"<question text\\><image\\> \\-\\- <answer text\\>\"\n",
    "- Taking data sets in previous step, with text-only LLM, to train an Image Projector. The image Projector and text-only LLM are called LLaVA model\n",
    "\n",
    "#### Deploy\n",
    "- Pretrain: use images with title and texts and text-only LLM (i.e. InterLM2_chat_1.8B) to generate a pretrained LLaVA\n",
    "- Finetune: use the pretrained LLaVA and images with complicated chats to generate Finetuned LLaVA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
